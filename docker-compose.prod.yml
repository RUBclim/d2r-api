services:
  db:
    restart: always
    env_file:
      - .env.prod
    deploy:
      resources:
        limits:
          cpus: "4"

  app:
    restart: always
    env_file:
      - .env.prod
    # run two workers per container, and 2 replicas
    command: [
      wait-for-it, "db:5432", --,
      gunicorn,
      "--bind", "0.0.0.0:5000",
      "--workers", "2",
      "--log-level", "info",
      "app.main:app",
      "-k", "uvicorn_worker.UvicornWorker",
    ]
    depends_on:
      db:
        condition: service_healthy
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          memory: 350M

  redis:
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: always
    # This file is created during deploy via ansible and the password is added
    volumes:
      - ./redis.conf:/usr/local/etc/redis/redis.conf:ro
    # we need to publish these ports, so we can access the redis server from rub-gis16
    ports:
      - "6379:6379"
    deploy:
      resources:
        limits:
          cpus: "0.75"
          memory: 250M

  celery:
    restart: always
    env_file:
      - .env.prod
    command: [
      celery, -A, app.celery, worker,
      --prefetch-multiplier=2,
      # a higher concurrency would prevent potential deadlocks caused by groups of
      # chains, however, processing the COGs is very memory intensive and we can
      # limit the memory usage by limiting the concurrency. We try to distribute
      # the memory usage by rate-limiting the task. This still allows task w/o rate
      # limiting to be processed in parallel in between.
      --concurrency=3,
    ]
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          # processing the COGs is memory intensive
          memory: 2048M

  celery-beat:
    env_file:
      - .env.prod
    command: [
      celery, -A, app.tasks, beat,
      --loglevel, INFO,
      --schedule, /tmp/celerybeat-schedule.db
    ]
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 200M

  terracotta-db:
    restart: always
    env_file:
      - .env.prod
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 200M

  terracotta-server:
    env_file:
      - .env.prod
    command: [
      gunicorn,
      "--bind", "0.0.0.0:5000",
      "--workers", "3",
      "--log-level", "info",
      "app.tc_app:app",
    ]
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          cpus: "2"
          memory: 1536M

  dashboard:
    restart: always
    deploy:
      mode: replicated
      replicas: 2
      resources:
        limits:
          memory: 400M

  nginx:
    restart: always
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      app:
        condition: service_healthy
    volumes:
      - ./nginx.prod.conf:/etc/nginx/conf.d/nginx.conf:ro
      # we need to override this to enable caching of the tiles
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ${NGINX_CERT_DIR}:/etc/nginx/certs:ro
    deploy:
      resources:
        limits:
          memory: 100M
